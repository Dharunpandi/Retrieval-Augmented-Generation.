# -*- coding: utf-8 -*-
"""rag_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VeS9HRiNBaBfT3Kq7z1FaXo1vRrEDLHE
"""

!pip install --upgrade --quiet langchain-community langchainhub langchain-chroma beautifulsoup4

!pip install -q langchain_google_genai

import os

os.environ['LANGCHAIN_TRAKING_V2']="true"
os.environ['LANGCHAIN_ENDPOINT']="https://api.smith.langchain.com"
os.environ['LANGCHAIN_API_KEY']=""
os.environ['LANGCHAIN_PROJECT']='RAG'
os.environ['GOOGLE_API_KEY']=''

import warnings
warnings.filterwarnings('ignore')

from langchain_google_genai import GoogleGenerativeAIEmbeddings
gemini_embeddings = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
)

from langchain_google_genai import GoogleGenerativeAIEmbeddings
gemini_embeddings = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
)

from langchain_google_genai import ChatGoogleGenerativeAI
model= ChatGoogleGenerativeAI(model="gemini-2.5-pro",convert_system_message_to_human=True)

print(model.invoke("hi").content)

import bs4
from langchain import hub

from langchain.chains import create_retrieval_chain

from langchain.chains.combine_documents import create_stuff_documents_chain

from langchain_chroma import Chroma

from langchain_community.document_loaders import WebBaseLoader

from langchain_core.prompts import ChatPromptTemplate

from langchain_text_splitters import RecursiveCharacterTextSplitter

from langchain_core.prompts import MessagesPlaceholder

loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
    bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=("post-content", "post-title", "post-header"))),
)

doc = loader.load()

doc

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
)

splits = text_splitter.split_documents(doc)

splits

vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=gemini_embeddings,
)

retriever = vectorstore.as_retriever()

system_prompt = (
    "You are an assistant for question answering tasks. "
    "Use the following pieces of retrieved context to answer the question "
    "If you don't know the answer, say that you don't know."
    "Use three sentences maximum and keep the answer concise."
    "\n\n"
    "{context}"
)

chat_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}"),
    ]
)

question_answering_chain = create_stuff_documents_chain(model, chat_prompt)

rag_chain = create_retrieval_chain(retriever, question_answering_chain)

rag_chain.invoke({"input": "What is MRKL?"})

